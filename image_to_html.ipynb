{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import CLIPProcessor, GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from model import ImageToHTMLModel"
      ],
      "metadata": {
        "id": "_ln5lO5sDpb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(example, processor, tokenizer):\n",
        "    image = processor(images=example['image'], return_tensors=\"pt\").pixel_values\n",
        "    html = tokenizer(example['html'], truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return {\"pixel_values\": image.squeeze(), \"labels\": html.input_ids.squeeze()}\n",
        "\n",
        "def train(model, train_dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(pixel_values, labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"HuggingFaceM4/WebSight\", split=\"train[:1000]\")  # Using a subset for demonstration\n",
        "\n",
        "    # Initialize models and processors\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "    # Preprocess dataset\n",
        "    processed_dataset = dataset.map(\n",
        "        lambda example: preprocess_data(example, clip_processor, gpt_tokenizer),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    # Create data loader\n",
        "    train_dataloader = DataLoader(processed_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImageToHTMLModel().to(device)\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss = train(model, train_dataloader, optimizer, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"image_to_html_model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "d-_7TQ43Dj_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}